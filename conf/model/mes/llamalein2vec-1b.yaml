module:
  _target_: models.pl_modules.BasePLModule
  RAdam:
    _target_: torch.optim.RAdam
    lr: 2e-5
  Adafactor:
    _target_: transformers.Adafactor
    lr: 3e-5
    weight_decay: 0.01
    scale_parameter: False
    relative_step: False
  lr_scheduler:     
    num_warmup_steps: 6381 # 6381 for tuba10, 6265 for se10
    num_training_steps: 80000 # 80000 default, 50000 for se10?
  opt: "Adafactor" #RAdam
  model:
    _target_: models.model_mes.Maverick_mes
    language_model: "llamalein2vec-1b"
    huggingface_model_name: "replace me with HF model name or local path"
    huggingface_revision: None
    freeze_encoder: False
    span_representation: "concat_start_end"
    use_attn: False
    teacher_forcing: False
    use_reps: False